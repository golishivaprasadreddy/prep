
1. During the learning phase, the interconnection weights are adjusted based on the ____
a) bias
b) epochs
c) Errors
d) All of the above

✅ Answer: c) Errors


---

2. A binary sigmoid function has output in the range of ___
a) -1,+1
b) 0,-1
c) 0,1
d) 1,0

✅ Answer: c) 0,1


---

3. Convolutional Neural Networks
a) Are similar to multilayer feed forward networks
b) May have self loops
c) Have feed back loops
d) All of the above

✅ Answer: a) Are similar to multilayer feed forward networks


---

4. In CNN, with flattening, we lose ____ within the data.
a) Structure
b) edges
c) Spatial relationships
d) None

✅ Answer: c) Spatial relationships


---

5. Weights should be initialized with ____ values.
a) Random
b) Constant
c) 0
d) None of the above

✅ Answer: a) Random


---

6. For an image of size 125 × 123 × 3, the number of input features will be equivalent to __________

Calculation:
125 × 123 × 3 = 46,125

✅ Answer: 46,125


---

7. In NN, each epoch consists of _______, _____ phases

✅ Answer: Forward phase, Backward phase


---

8. ______________ is used to extract features from the input image.

✅ Answer: Convolution layer (Filters/Kernels)


---

9. If the problem is binary classification, then the number of neurons in the output layer are ____
a) 1
b) 2
c) depends on activation
d) None

✅ Answer: a) 1


---

10. The output values of the Tanh function are in the range of _______
a) -1 to +1
b) 0 to 1
c) -∞ to +∞
d) None

✅ Answer: a) -1 to +1


---
   **set 2 **

. A Sigmoid function has range of ___
a) -1,+1
b) 0,-1
c) 0,1
d) 1,0

✅ Answer: c) 0,1


---

2. α is the ___ of the neural network
a) Growth rate
b) Multiplier
c) Learning rate
d) None of the above

✅ Answer: c) Learning rate


---

3. To handle intense computation of deep learning ____ is needed.
a) Parallel computing
b) CPU based traditional computing
c) GPU computing
d) None of the above

✅ Answer: c) GPU computing
(Parallel computing with GPUs is the most common in deep learning)


---

4. Single layer perceptron is able to deal with
a) Linearly separable data
b) Non Linearly separable data
c) Linearly inseparable data
d) None of the above

✅ Answer: a) Linearly separable data


---

5. For a deep neural network, if large values are used to initialize weights, the gradient
a) Explodes
b) Vanishes
c) Becomes 0
d) None of the above

✅ Answer: a) Explodes


---

6. The most popularly used activation function for the hidden layers in deep CNN is __________

✅ Answer: ReLU (Rectified Linear Unit)


---

7. Deep Learning is the subset of _____

✅ Answer: Machine Learning


---

8. During training of neural network model, Error or loss = _______

✅ Answer: Difference between actual output and predicted output


---

9. Single layer feed forward network consists of ____ layers

✅ Answer: 2 layers (Input layer + Output layer)


---

10. Neural networks mimic ________

✅ Answer: Human brain (Biological neural networks / neurons)

**set 3**
Weights should be initialized with ____ values.
a) Random
b) Constant
c) 0
d) None of the above

✅ Answer: a) Random


---

2. It is a good idea to start training a neural network by initializing the bias with 0 values.
a) True
b) False
c) None
d) Can’t tell

✅ Answer: a) True


---

3. α is the ___ of the neural network
a) Growth rate
b) Multiplier
c) Learning rate
d) None of the above

✅ Answer: c) Learning rate


---

4. Single layer perceptron is able to deal with
a) Linearly separable data
b) Non Linearly separable data
c) Linearly inseparable data
d) None of the above

✅ Answer: a) Linearly separable data


---

5. To handle intense computation of deep learning ____ is needed.
a) Parallel computing
b) CPU based traditional computing
c) GPU computing
d) None of the above

✅ Answer: c) GPU computing


---

6. Multi layer feed forward network consists of ____ layers

✅ Answer: At least 3 layers (Input layer + Hidden layer(s) + Output layer)


---

7. For balanced weight initialization, ___________ initialization can be used.

✅ Answer: Xavier (or Glorot) initialization


---

8. Stochastic Gradient Descent works based on ________ training records.

✅ Answer: Single training record (one at a time)


---

9. The final layer of a CNN is called the _________ layer.

✅ Answer: Fully Connected (FC) layer / Output layer


---

10. For a particular mini batch gradient descent with n training records and a batch size of m, there will be ___ mini batches.

✅ Answer: n / m mini batches

**Set 4**

1. Deep neural networks generally have more than ____ hidden layers.
a) 1
b) 2
c) 3
d) None of the above

✅ Answer: a) 1
(Any network with more than 1 hidden layer is considered “deep”)


---

2. Which of the following is a valid hyper parameter?
a) Learning rate
b) No of layers
c) Mini batch size
d) All of the above

✅ Answer: d) All of the above


---

3. Adam optimizer is a combination of ____ and ____
a) RMSProp and Adagrad
b) RMSProp and gradient descent with momentum
c) Adagrad and Adadelta
d) RMSProp and Adadelta

✅ Answer: b) RMSProp and gradient descent with momentum


---

4. Which of the following problem(s) is faced by gradient descent?
a) Saddle point
b) Local minima
c) Both a & b
d) None of the above

✅ Answer: c) Both a & b


---

5. Dropout regularization can be applied to the output layer
a) True
b) False
c) None
d) Can’t tell

✅ Answer: a) True
(though more common in hidden layers, it can be applied to the output layer)


---

6. Mini Batch Gradient Descent works based on ________ training records

✅ Answer: Small random subsets of training records (mini batches)


---

7. In CNN, stride corresponds to __________________

✅ Answer: The step size by which the filter/kernel moves over the input image


---

8. In CNN, with the usage of 3 kernels, ____ no of feature maps are generated.

✅ Answer: 3 feature maps


---

9. Batch Gradient Descent works based on ________ training records

✅ Answer: All training records (entire dataset at once)


---

10. ReLU stands for __________

✅ Answer: Rectified Linear Unit
